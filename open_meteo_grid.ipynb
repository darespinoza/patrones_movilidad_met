{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5701c7ff",
   "metadata": {},
   "source": [
    "# OpenMeteo DuckDB Cache\n",
    "Claude 3.7 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2415f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "import duckdb\n",
    "\n",
    "class OpenMeteoDuckDBCache:\n",
    "    \"\"\"\n",
    "    A class to efficiently query the OpenMeteo API by avoiding redundant calls\n",
    "    for locations within the same grid cell/pixel, using DuckDB for storage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path='openmeteo_cache.duckdb', grid_resolution=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the OpenMeteo cache system with DuckDB storage.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        db_path : str\n",
    "            Path to the DuckDB database file\n",
    "        grid_resolution : float\n",
    "            The resolution of the OpenMeteo grid in degrees (default is 0.1Â°)\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.grid_resolution = grid_resolution\n",
    "        self.memory_cache = {}\n",
    "        \n",
    "        # Initialize the DuckDB connection\n",
    "        self.conn = duckdb.connect(db_path)\n",
    "        \n",
    "        # Create necessary tables if they don't exist\n",
    "        self._initialize_database()\n",
    "    \n",
    "    def _initialize_database(self):\n",
    "        \"\"\"\n",
    "        Set up the database schema for the cache.\n",
    "        \"\"\"\n",
    "        # Create cache metadata table to store query information\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS cache_metadata (\n",
    "                cache_key VARCHAR PRIMARY KEY,\n",
    "                grid_lat DOUBLE,\n",
    "                grid_lon DOUBLE,\n",
    "                start_date DATE,\n",
    "                end_date DATE,\n",
    "                variables VARCHAR,\n",
    "                created_at TIMESTAMP,\n",
    "                last_accessed TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create cache data table to store actual weather data\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS cache_data (\n",
    "                cache_key VARCHAR,\n",
    "                timestamp TIMESTAMP,\n",
    "                variable VARCHAR,\n",
    "                value DOUBLE,\n",
    "                PRIMARY KEY (cache_key, timestamp, variable),\n",
    "                FOREIGN KEY (cache_key) REFERENCES cache_metadata(cache_key)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create index for faster retrieval\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_grid_location \n",
    "            ON cache_metadata(grid_lat, grid_lon, start_date, end_date)\n",
    "        \"\"\")\n",
    "    \n",
    "    def _get_grid_cell(self, lat, lon):\n",
    "        \"\"\"\n",
    "        Convert exact coordinates to grid cell coordinates by rounding to \n",
    "        the nearest grid point based on the resolution.\n",
    "        \"\"\"\n",
    "        grid_lat = round(lat / self.grid_resolution) * self.grid_resolution\n",
    "        grid_lon = round(lon / self.grid_resolution) * self.grid_resolution\n",
    "        return (grid_lat, grid_lon)\n",
    "    \n",
    "    def _create_cache_key(self, grid_cell, start_date, end_date, variables):\n",
    "        \"\"\"\n",
    "        Create a unique cache key based on location and query parameters.\n",
    "        \"\"\"\n",
    "        # Sort variables to ensure consistent key regardless of order\n",
    "        if isinstance(variables, list):\n",
    "            variables = sorted(variables)\n",
    "        \n",
    "        # Create a string that represents all query parameters\n",
    "        params_str = f\"{grid_cell}_{start_date}_{end_date}_{','.join(variables)}\"\n",
    "        \n",
    "        # Create a hash for the parameters\n",
    "        return hashlib.md5(params_str.encode()).hexdigest()\n",
    "    \n",
    "    def _check_cache(self, grid_lat, grid_lon, start_date, end_date, variables):\n",
    "        \"\"\"\n",
    "        Check if data for the given parameters exists in cache.\n",
    "        \"\"\"\n",
    "        variables_str = ','.join(sorted(variables)) if isinstance(variables, list) else variables\n",
    "        \n",
    "        result = self.conn.execute(\"\"\"\n",
    "            SELECT cache_key \n",
    "            FROM cache_metadata \n",
    "            WHERE grid_lat = ? AND grid_lon = ? \n",
    "                AND start_date <= ? AND end_date >= ?\n",
    "                AND variables = ?\n",
    "        \"\"\", [grid_lat, grid_lon, start_date, end_date, variables_str]).fetchone()\n",
    "        \n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def _store_in_cache(self, cache_key, grid_lat, grid_lon, start_date, end_date, variables, data):\n",
    "        \"\"\"\n",
    "        Store API response in the DuckDB cache.\n",
    "        \"\"\"\n",
    "        variables_str = ','.join(sorted(variables)) if isinstance(variables, list) else variables\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Store metadata\n",
    "        self.conn.execute(\"\"\"\n",
    "            INSERT INTO cache_metadata (\n",
    "                cache_key, grid_lat, grid_lon, start_date, end_date, \n",
    "                variables, created_at, last_accessed\n",
    "            )\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", [cache_key, grid_lat, grid_lon, start_date, end_date, \n",
    "              variables_str, now, now])\n",
    "        \n",
    "        # Process and store hourly data\n",
    "        if 'hourly' in data:\n",
    "            hourly_times = data['hourly']['time']\n",
    "            \n",
    "            # Prepare batch insert data\n",
    "            insert_data = []\n",
    "            \n",
    "            for var in variables:\n",
    "                if var in data['hourly']:\n",
    "                    hourly_values = data['hourly'][var]\n",
    "                    \n",
    "                    for i, time_str in enumerate(hourly_times):\n",
    "                        timestamp = datetime.fromisoformat(time_str.replace('Z', '+00:00'))\n",
    "                        insert_data.append((cache_key, timestamp, var, hourly_values[i]))\n",
    "            \n",
    "            # Batch insert all data\n",
    "            if insert_data:\n",
    "                self.conn.executemany(\"\"\"\n",
    "                    INSERT INTO cache_data (cache_key, timestamp, variable, value)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                \"\"\", insert_data)\n",
    "        \n",
    "        # Store in memory cache too for faster access\n",
    "        self.memory_cache[cache_key] = data\n",
    "    \n",
    "    def _retrieve_from_cache(self, cache_key, variables):\n",
    "        \"\"\"\n",
    "        Retrieve data from the DuckDB cache and format it like the OpenMeteo API response.\n",
    "        \"\"\"\n",
    "        # Update last accessed timestamp\n",
    "        self.conn.execute(\"\"\"\n",
    "            UPDATE cache_metadata \n",
    "            SET last_accessed = ? \n",
    "            WHERE cache_key = ?\n",
    "        \"\"\", [datetime.now(), cache_key])\n",
    "        \n",
    "        # Check memory cache first (faster)\n",
    "        if cache_key in self.memory_cache:\n",
    "            return self.memory_cache[cache_key]\n",
    "        \n",
    "        # Query the database for all data with this cache key\n",
    "        result = self.conn.execute(\"\"\"\n",
    "            SELECT timestamp, variable, value\n",
    "            FROM cache_data\n",
    "            WHERE cache_key = ?\n",
    "            ORDER BY timestamp, variable\n",
    "        \"\"\", [cache_key]).fetchdf()\n",
    "        \n",
    "        if result.empty:\n",
    "            return None\n",
    "        \n",
    "        # Reconstruct the API response format\n",
    "        response = {\"hourly\": {\"time\": []}}\n",
    "        \n",
    "        # Group by timestamp to organize data\n",
    "        grouped = result.groupby('timestamp')\n",
    "        \n",
    "        # Format timestamps and prepare data structure\n",
    "        for timestamp, group in grouped:\n",
    "            time_str = timestamp.isoformat().replace('+00:00', 'Z')\n",
    "            \n",
    "            # Add timestamp to the list if it's not already there\n",
    "            if time_str not in response[\"hourly\"][\"time\"]:\n",
    "                response[\"hourly\"][\"time\"].append(time_str)\n",
    "            \n",
    "            # For each variable in this timestamp group, add to response\n",
    "            for _, row in group.iterrows():\n",
    "                var = row['variable']\n",
    "                val = row['value']\n",
    "                \n",
    "                if var not in response[\"hourly\"]:\n",
    "                    response[\"hourly\"][var] = []\n",
    "                \n",
    "                # Ensure we're adding in the correct position\n",
    "                time_index = response[\"hourly\"][\"time\"].index(time_str)\n",
    "                \n",
    "                # Expand list if needed\n",
    "                while len(response[\"hourly\"][var]) <= time_index:\n",
    "                    response[\"hourly\"][var].append(None)\n",
    "                \n",
    "                response[\"hourly\"][var][time_index] = val\n",
    "        \n",
    "        # Store in memory cache for faster future access\n",
    "        self.memory_cache[cache_key] = response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_weather_data(self, lat, lon, start_date, end_date, variables=None, force_refresh=False):\n",
    "        \"\"\"\n",
    "        Get weather data for a specific location and time range.\n",
    "        Checks cache first to avoid redundant API calls.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lat : float\n",
    "            Latitude of the location\n",
    "        lon : float\n",
    "            Longitude of the location\n",
    "        start_date : str\n",
    "            Start date in YYYY-MM-DD format\n",
    "        end_date : str\n",
    "            End date in YYYY-MM-DD format\n",
    "        variables : list\n",
    "            List of weather variables to retrieve (default: temperature and precipitation)\n",
    "        force_refresh : bool\n",
    "            If True, force a fresh API call even if data is in cache\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Weather data from OpenMeteo API or cache\n",
    "        \"\"\"\n",
    "        if variables is None:\n",
    "            variables = [\"temperature_2m\", \"precipitation\"]\n",
    "        \n",
    "        # Determine which grid cell this location falls into\n",
    "        grid_cell = self._get_grid_cell(lat, lon)\n",
    "        grid_lat, grid_lon = grid_cell\n",
    "        \n",
    "        # Create cache key\n",
    "        cache_key = self._create_cache_key(grid_cell, start_date, end_date, variables)\n",
    "        \n",
    "        if not force_refresh:\n",
    "            # Check if data exists in cache\n",
    "            existing_key = self._check_cache(grid_lat, grid_lon, start_date, end_date, variables)\n",
    "            \n",
    "            if existing_key:\n",
    "                print(f\"Using cache for {grid_cell} from {start_date} to {end_date}\")\n",
    "                return self._retrieve_from_cache(existing_key, variables)\n",
    "        \n",
    "        # If not in cache or force refresh, call the API\n",
    "        print(f\"Calling OpenMeteo API for {grid_cell} from {start_date} to {end_date}\")\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Construct the API URL\n",
    "        base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": grid_lat,\n",
    "            \"longitude\": grid_lon,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": variables,\n",
    "            \"timezone\": \"auto\"\n",
    "        }\n",
    "        \n",
    "        # Make the request\n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Save to cache\n",
    "            self._store_in_cache(\n",
    "                cache_key, grid_lat, grid_lon, \n",
    "                start_date, end_date, variables, data\n",
    "            )\n",
    "            \n",
    "            return data\n",
    "        else:\n",
    "            print(f\"API request failed with status code {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "    \n",
    "    def process_mobility_dataset(self, df, date_column='timestamp', lat_column='latitude', \n",
    "                                lon_column='longitude', variables=None, batch_size=1000):\n",
    "        \"\"\"\n",
    "        Process a mobility dataset, adding weather data to each point efficiently.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Mobility dataset with timestamp and coordinates\n",
    "        date_column : str\n",
    "            Name of the column containing timestamps\n",
    "        lat_column : str\n",
    "            Name of the column containing latitude\n",
    "        lon_column : str\n",
    "            Name of the column containing longitude\n",
    "        variables : list\n",
    "            Weather variables to retrieve\n",
    "        batch_size : int\n",
    "            Number of records to process in each batch (for large datasets)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Original dataframe with added weather data columns\n",
    "        \"\"\"\n",
    "        if variables is None:\n",
    "            variables = [\"temperature_2m\", \"precipitation\"]\n",
    "        \n",
    "        # Create a copy to avoid modifying the original\n",
    "        df_result = df.copy()\n",
    "        \n",
    "        # Ensure timestamp is datetime\n",
    "        df_result[date_column] = pd.to_datetime(df_result[date_column])\n",
    "        \n",
    "        # Add grid cell columns for grouping\n",
    "        df_result['grid_lat'] = df_result[lat_column].apply(\n",
    "            lambda x: round(x / self.grid_resolution) * self.grid_resolution\n",
    "        )\n",
    "        df_result['grid_lon'] = df_result[lon_column].apply(\n",
    "            lambda x: round(x / self.grid_resolution) * self.grid_resolution\n",
    "        )\n",
    "        \n",
    "        # Extract date strings needed for API\n",
    "        df_result['date'] = df_result[date_column].dt.date.astype(str)\n",
    "        df_result['hour'] = df_result[date_column].dt.hour\n",
    "        \n",
    "        # Initialize weather data columns\n",
    "        for var in variables:\n",
    "            df_result[var] = np.nan\n",
    "        \n",
    "        # Process by unique grid cell and date combinations\n",
    "        unique_cells = df_result[['grid_lat', 'grid_lon', 'date']].drop_duplicates()\n",
    "        \n",
    "        print(f\"Processing {len(unique_cells)} unique grid cell/date combinations\")\n",
    "        \n",
    "        # Process in batches for large datasets\n",
    "        total_batches = (len(unique_cells) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_num in range(total_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, len(unique_cells))\n",
    "            \n",
    "            batch = unique_cells.iloc[start_idx:end_idx]\n",
    "            print(f\"Processing batch {batch_num+1}/{total_batches}\")\n",
    "            \n",
    "            for idx, row in batch.iterrows():\n",
    "                grid_lat = row['grid_lat']\n",
    "                grid_lon = row['grid_lon']\n",
    "                date = row['date']\n",
    "                \n",
    "                # Each API call covers one day\n",
    "                weather_data = self.get_weather_data(\n",
    "                    grid_lat, grid_lon, date, date, variables\n",
    "                )\n",
    "                \n",
    "                if weather_data and 'hourly' in weather_data:\n",
    "                    # Create a mapping of hour to weather values for this day\n",
    "                    hour_to_values = {}\n",
    "                    \n",
    "                    for hour_idx, time_str in enumerate(weather_data['hourly']['time']):\n",
    "                        hour = int(time_str.split('T')[1].split(':')[0])\n",
    "                        hour_values = {}\n",
    "                        \n",
    "                        for var in variables:\n",
    "                            if var in weather_data['hourly'] and hour_idx < len(weather_data['hourly'][var]):\n",
    "                                hour_values[var] = weather_data['hourly'][var][hour_idx]\n",
    "                        \n",
    "                        hour_to_values[hour] = hour_values\n",
    "                    \n",
    "                    # Update dataframe for this grid cell and date\n",
    "                    mask = (\n",
    "                        (df_result['grid_lat'] == grid_lat) & \n",
    "                        (df_result['grid_lon'] == grid_lon) & \n",
    "                        (df_result['date'] == date)\n",
    "                    )\n",
    "                    \n",
    "                    # Apply values based on hour\n",
    "                    for hour, values in hour_to_values.items():\n",
    "                        hour_mask = mask & (df_result['hour'] == hour)\n",
    "                        \n",
    "                        for var, value in values.items():\n",
    "                            df_result.loc[hour_mask, var] = value\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        df_result.drop(['grid_lat', 'grid_lon', 'date', 'hour'], axis=1, inplace=True)\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    def optimize_database(self):\n",
    "        \"\"\"\n",
    "        Optimize the database by removing duplicates and vacuum-analyzing.\n",
    "        \"\"\"\n",
    "        # Find duplicate data and keep only the most recent\n",
    "        self.conn.execute(\"\"\"\n",
    "            DELETE FROM cache_data \n",
    "            WHERE rowid NOT IN (\n",
    "                SELECT MAX(rowid) \n",
    "                FROM cache_data \n",
    "                GROUP BY cache_key, timestamp, variable\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Find orphaned data (without metadata) and remove it\n",
    "        self.conn.execute(\"\"\"\n",
    "            DELETE FROM cache_data \n",
    "            WHERE cache_key NOT IN (\n",
    "                SELECT cache_key FROM cache_metadata\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Vacuum and analyze to reclaim space and optimize\n",
    "        self.conn.execute(\"VACUUM\")\n",
    "        self.conn.execute(\"ANALYZE\")\n",
    "    \n",
    "    def clear_cache(self, older_than_days=None):\n",
    "        \"\"\"\n",
    "        Clear the cache, optionally only removing entries older than specified days.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        older_than_days : int or None\n",
    "            If specified, only clear cache entries older than this many days\n",
    "        \"\"\"\n",
    "        if older_than_days is not None:\n",
    "            cutoff_date = datetime.now() - timedelta(days=older_than_days)\n",
    "            \n",
    "            # Get keys to remove from memory cache\n",
    "            keys_to_remove = self.conn.execute(\"\"\"\n",
    "                SELECT cache_key \n",
    "                FROM cache_metadata \n",
    "                WHERE created_at < ?\n",
    "            \"\"\", [cutoff_date]).fetchall()\n",
    "            \n",
    "            # Delete old data\n",
    "            self.conn.execute(\"\"\"\n",
    "                DELETE FROM cache_data \n",
    "                WHERE cache_key IN (\n",
    "                    SELECT cache_key \n",
    "                    FROM cache_metadata \n",
    "                    WHERE created_at < ?\n",
    "                )\n",
    "            \"\"\", [cutoff_date])\n",
    "            \n",
    "            self.conn.execute(\"\"\"\n",
    "                DELETE FROM cache_metadata \n",
    "                WHERE created_at < ?\n",
    "            \"\"\", [cutoff_date])\n",
    "            \n",
    "            # Remove from memory cache\n",
    "            for key in keys_to_remove:\n",
    "                if key[0] in self.memory_cache:\n",
    "                    del self.memory_cache[key[0]]\n",
    "        else:\n",
    "            # Clear all data\n",
    "            self.conn.execute(\"DELETE FROM cache_data\")\n",
    "            self.conn.execute(\"DELETE FROM cache_metadata\")\n",
    "            self.memory_cache = {}\n",
    "        \n",
    "        # Vacuum the database to reclaim space\n",
    "        self.conn.execute(\"VACUUM\")\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"\n",
    "        Get statistics about the cache.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Statistics about the cache\n",
    "        \"\"\"\n",
    "        # Total entries\n",
    "        total_entries = self.conn.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM cache_metadata\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        # Data points\n",
    "        data_points = self.conn.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM cache_data\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        # Unique grid cells\n",
    "        unique_grid_cells = self.conn.execute(\"\"\"\n",
    "            SELECT COUNT(DISTINCT (grid_lat || ',' || grid_lon)) \n",
    "            FROM cache_metadata\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        # Date range\n",
    "        date_range = self.conn.execute(\"\"\"\n",
    "            SELECT MIN(start_date), MAX(end_date) \n",
    "            FROM cache_metadata\n",
    "        \"\"\").fetchone()\n",
    "        \n",
    "        # Most queried locations\n",
    "        popular_locations = self.conn.execute(\"\"\"\n",
    "            SELECT grid_lat, grid_lon, COUNT(*) AS query_count\n",
    "            FROM cache_metadata\n",
    "            GROUP BY grid_lat, grid_lon\n",
    "            ORDER BY query_count DESC\n",
    "            LIMIT 5\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        return {\n",
    "            \"total_entries\": total_entries,\n",
    "            \"data_points\": data_points,\n",
    "            \"unique_grid_cells\": unique_grid_cells,\n",
    "            \"date_range\": date_range,\n",
    "            \"popular_locations\": popular_locations\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the database connection properly.\n",
    "        \"\"\"\n",
    "        if self.conn:\n",
    "            self.optimize_database()\n",
    "            self.conn.close()\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create a sample mobility dataset\n",
    "#     data = {\n",
    "#         'user_id': range(1, 101),\n",
    "#         'timestamp': pd.date_range(start='2023-01-01', periods=100, freq='H'),\n",
    "#         'latitude': np.random.uniform(40.0, 41.0, 100),  # Sample coordinates for Madrid\n",
    "#         'longitude': np.random.uniform(-3.8, -3.6, 100),\n",
    "#         'activity': np.random.choice(['walking', 'cycling', 'driving'], 100)\n",
    "#     }\n",
    "    \n",
    "#     mobility_df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Initialize the cache with DuckDB\n",
    "#     cache = OpenMeteoDuckDBCache(db_path='openmeteo_cache.duckdb', grid_resolution=0.1)\n",
    "    \n",
    "#     try:\n",
    "#         # Process the mobility dataset to add weather data\n",
    "#         print(\"\\nProcessing mobility dataset...\")\n",
    "#         result_df = cache.process_mobility_dataset(\n",
    "#             mobility_df,\n",
    "#             variables=[\"temperature_2m\", \"precipitation\", \"windspeed_10m\"]\n",
    "#         )\n",
    "        \n",
    "#         # Show the result\n",
    "#         print(\"\\nSample of processed data:\")\n",
    "#         print(result_df.head())\n",
    "        \n",
    "#         # Get cache statistics\n",
    "#         stats = cache.get_cache_stats()\n",
    "#         print(\"\\nCache statistics:\")\n",
    "#         print(f\"Total cache entries: {stats['total_entries']}\")\n",
    "#         print(f\"Total data points stored: {stats['data_points']}\")\n",
    "#         print(f\"Unique grid cells: {stats['unique_grid_cells']}\")\n",
    "#         print(f\"Date range: {stats['date_range']}\")\n",
    "#         print(\"\\nMost queried locations:\")\n",
    "#         print(stats['popular_locations'])\n",
    "        \n",
    "#         # Show efficiency statistics\n",
    "#         unique_points = len(mobility_df)\n",
    "#         unique_cells = stats['unique_grid_cells']\n",
    "        \n",
    "#         print(\"\\nEfficiency statistics:\")\n",
    "#         print(f\"Total data points: {unique_points}\")\n",
    "#         print(f\"API calls made: {unique_cells}\")\n",
    "#         print(f\"API calls saved: {unique_points - unique_cells}\")\n",
    "#         if unique_points > 0:\n",
    "#             print(f\"Efficiency: {(1 - unique_cells/unique_points) * 100:.1f}% reduction in API calls\")\n",
    "        \n",
    "#     finally:\n",
    "#         # Always close the database connection properly\n",
    "#         cache.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
